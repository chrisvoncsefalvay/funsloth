{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth SFT Fine-tuning Template\n",
    "\n",
    "This notebook provides a complete template for supervised fine-tuning (SFT) with Unsloth.\n",
    "\n",
    "**What you'll learn:**\n",
    "- How to load quantized models efficiently with Unsloth\n",
    "- Configuring LoRA for parameter-efficient fine-tuning\n",
    "- Training with SFTTrainer from TRL\n",
    "- Saving and exporting your fine-tuned model\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 8GB+ VRAM (adjust batch_size if limited)\n",
    "- ~30 minutes for a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "Run this cell to install/update Unsloth and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Get latest Unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    raise RuntimeError(\"No GPU detected! Training requires CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "**Modify these values** to customize your training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Modify these values\n",
    "# ============================================\n",
    "\n",
    "# Model - see references/model_selection.md for options\n",
    "MODEL_NAME = \"unsloth/llama-3.1-8b-unsloth-bnb-4bit\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"mlabonne/FineTome-100k\"\n",
    "DATASET_FORMAT = \"sharegpt\"  # Options: alpaca, sharegpt, chatml, raw\n",
    "CHAT_TEMPLATE = \"llama-3.1\"  # Must match model family\n",
    "\n",
    "# LoRA Configuration\n",
    "LORA_RANK = 16         # Higher = more capacity, more VRAM\n",
    "LORA_ALPHA = 16        # Usually equal to rank\n",
    "LORA_DROPOUT = 0       # 0 is fine for most cases\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE = 2                 # Reduce if OOM\n",
    "GRADIENT_ACCUMULATION = 4      # Effective batch = BATCH_SIZE * GRAD_ACCUM\n",
    "LEARNING_RATE = 2e-4           # Unsloth recommended\n",
    "NUM_EPOCHS = 1                 # 1-3 for instruction tuning\n",
    "WARMUP_STEPS = 5\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "\n",
    "print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model\n",
    "\n",
    "Unsloth optimizes the model loading process for 2x faster training and 70% less memory usage with 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=None,  # Auto-detect best dtype\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA Adapters\n",
    "\n",
    "LoRA (Low-Rank Adaptation) adds small trainable matrices to the model. This allows fine-tuning with a fraction of the parameters.\n",
    "\n",
    "**Key parameters:**\n",
    "- `r` (rank): Higher = more capacity but more memory. 16-64 is typical.\n",
    "- `lora_alpha`: Scaling factor, usually equal to r.\n",
    "- `target_modules`: Which layers to adapt. More modules = better adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP\n",
    "    ],\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable:,} ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Format Dataset\n",
    "\n",
    "This cell handles different dataset formats. The format is detected from the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "print(f\"Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "\n",
    "# Apply chat template\n",
    "tokenizer = get_chat_template(tokenizer, chat_template=CHAT_TEMPLATE)\n",
    "\n",
    "# Format based on dataset type\n",
    "if DATASET_FORMAT == \"sharegpt\":\n",
    "    # ShareGPT format needs standardization\n",
    "    dataset = standardize_sharegpt(dataset)\n",
    "    \n",
    "    def format_fn(example):\n",
    "        return {\"text\": tokenizer.apply_chat_template(\n",
    "            example[\"conversations\"], tokenize=False\n",
    "        )}\n",
    "\n",
    "elif DATASET_FORMAT == \"alpaca\":\n",
    "    def format_fn(example):\n",
    "        messages = [{\"role\": \"user\", \"content\": example[\"instruction\"]}]\n",
    "        if example.get(\"input\"):\n",
    "            messages[0][\"content\"] += f\"\\n\\n{example['input']}\"\n",
    "        messages.append({\"role\": \"assistant\", \"content\": example[\"output\"]})\n",
    "        return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False)}\n",
    "\n",
    "elif DATASET_FORMAT == \"chatml\":\n",
    "    def format_fn(example):\n",
    "        return {\"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], tokenize=False\n",
    "        )}\n",
    "\n",
    "else:  # raw\n",
    "    def format_fn(example):\n",
    "        return {\"text\": example[\"text\"]}\n",
    "\n",
    "# Apply formatting\n",
    "dataset = dataset.map(format_fn)\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nSample formatted text:\\n{dataset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "SFTTrainer (Supervised Fine-Tuning) teaches the model to generate responses that match your training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can enable for efficiency with short sequences\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        save_strategy=\"epoch\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(f\"\\nTraining complete! Final loss: {trainer_stats.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "Save the trained model in multiple formats for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only (small, can be merged later)\n",
    "model.save_pretrained(f\"{OUTPUT_DIR}/lora_adapter\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/lora_adapter\")\n",
    "print(f\"LoRA adapter saved to {OUTPUT_DIR}/lora_adapter\")\n",
    "\n",
    "# Save merged model in 16-bit (for full deployment)\n",
    "model.save_pretrained_merged(\n",
    "    f\"{OUTPUT_DIR}/merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "print(f\"Merged model saved to {OUTPUT_DIR}/merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Inference\n",
    "\n",
    "Quick test to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Can you introduce yourself?\"}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Optional: Save as GGUF\n",
    "\n",
    "Convert to GGUF for use with llama.cpp, Ollama, or LM Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate GGUF files\n",
    "# model.save_pretrained_gguf(\n",
    "#     f\"{OUTPUT_DIR}/gguf\",\n",
    "#     tokenizer,\n",
    "#     quantization_method=\"q4_k_m\",  # Options: q4_k_m, q5_k_m, q8_0\n",
    "# )\n",
    "# print(f\"GGUF saved to {OUTPUT_DIR}/gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Push to Hugging Face Hub\n",
    "\n",
    "Upload your model to share with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and set your repo name\n",
    "# HUB_REPO = \"your-username/your-model-name\"\n",
    "# \n",
    "# model.push_to_hub(HUB_REPO, token=True)\n",
    "# tokenizer.push_to_hub(HUB_REPO, token=True)\n",
    "# print(f\"Model pushed to https://huggingface.co/{HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Test your model** with various prompts\n",
    "2. **Convert to GGUF** if you want local inference\n",
    "3. **Upload to Hub** to share your model\n",
    "4. **Try DPO** for preference alignment (see dpo_template.ipynb)\n",
    "\n",
    "For more information, see the reference guides:\n",
    "- `references/model_selection.md` - Choosing base models\n",
    "- `references/hardware_guide.md` - VRAM requirements\n",
    "- `references/training_methods.md` - SFT vs DPO vs GRPO\n",
    "- `references/troubleshooting.md` - Common issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
